* Тема: Как сгенерировать русские имена?
* Цель: реализовать rnn с нуля, посмотреть как решать эту задачу через rnn и применить ее к своим данным, как обучить rnn и lstm в pytorch?
* Источники информации
[[https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.log_softmax][log_softmax]]
[[https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow][what is logit]]
[[https://mlexplained.com/2019/02/15/building-an-lstm-from-scratch-in-pytorch-lstms-in-depth-part-1/][Building an LSTM from Scratch in PyTorch (LSTMs in Depth Part 1)]]
[[https://www.youtube.com/watch?v=Keqep_PKrY8&t=1080s][Lecture 8: Recurrent Neural Networks and Language Models]]
[[https://peterroelants.github.io/posts/rnn-implementation-part01/][How to implement a simple RNN]]
[[https://stackoverflow.com/questions/50149049/training-a-rnn-in-pytorch][how to train rnn in pytorch (stackoverflow)]]
[[https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model][pytorch-tutorial for lstm language model]]
[[https://yaroshenko.by/char-lstm-text-generation/%09][Как научить нейросеть генерировать текст с помощью LSTM в PyTorch]]
https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html

https://pytorch.org/docs/stable/nn.html
https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms
https://github.com/gabrielloye/RNN-walkthrough/blob/master/main.ipynb

* Тезисы
** Как работать с скрытыми состояниями h_n, c_n, hidden? Когда их передавать, а когда - выкидывать?
https://stackoverflow.com/questions/56677052/is-hidden-and-output-the-same-for-a-gru-unit-in-pytorch/56683970#56683970
https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm
https://github.com/Samsung-IT-Academy/stepik-dl-nlp/blob/master/task4_RNN_name_generator.ipynb

Если батчи идут последовательно, как в [[https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model][pytorch-tutorial for lstm language model]], то подаем скрытые вектора обратно на вход модели. Если же
батчи не связаны между собой, как в https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch,
https://github.com/Samsung-IT-Academy/stepik-dl-nlp/blob/master/task4_RNN_name_generator.ipynb, то выкидывем скрытые вектора. Внутри батча
pytorch.nn.RNN и pytorch.nn.LSTM сами передают скрытые вектора.

** Как обучать RNN, если ей нужно подавать последоватеьные послежовательности и нельзя вроде как семплить случайно?
** Truncated backprop
https://discuss.pytorch.org/t/implementing-truncated-backpropagation-through-time/15500/3
