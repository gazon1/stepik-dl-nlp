{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle,\n",
    "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
    "\n",
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "# import sys; sys.path.append('./stepik-dl-nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:30.785285Z",
     "start_time": "2019-10-29T19:19:29.542846Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlnlputils\n",
    "from dlnlputils.data import tokenize_corpus, build_vocabulary, texts_to_token_ids, \\\n",
    "    PaddedSequenceDataset, Embeddings\n",
    "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "from dlnlputils.visualization import plot_vectors\n",
    "\n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных и подготовка корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:31.270503Z",
     "start_time": "2019-10-29T19:19:30.787789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка 125344\n",
      "Тестовая выборка 53719\n",
      "\n",
      "1/4 cup sour cream\n",
      "10 ounces swordfish, red snapper or other firm-fleshed fish\n",
      "1 tablespoon minced basil leaves\n",
      "Handful fresh parsley, finely minced\n",
      "4 ounces lard or butter, plus more for brushing tops\n",
      "4 to 5 green cardamom pods\n",
      "1 stick ( 1/4 pound) unsalted butter, softened\n",
      "1/4 teaspoon red pepper flakes, preferably Turkish or Aleppo (see note), more to taste\n",
      "1 tablespoon fresh lemon juice\n",
      "1/4 cup scallions, thinly sliced\n"
     ]
    }
   ],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "full_dataset = list(pd.read_csv('./datasets/nyt-ingredients-snapshot-2015.csv')['input'].dropna())\n",
    "random.shuffle(full_dataset)\n",
    "\n",
    "TRAIN_VAL_SPLIT = int(len(full_dataset) * 0.7)\n",
    "train_source = full_dataset[:TRAIN_VAL_SPLIT]\n",
    "test_source = full_dataset[TRAIN_VAL_SPLIT:]\n",
    "print(\"Обучающая выборка\", len(train_source))\n",
    "print(\"Тестовая выборка\", len(test_source))\n",
    "print()\n",
    "print('\\n'.join(train_source[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:32.137838Z",
     "start_time": "2019-10-29T19:19:31.272363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sour cream\n",
      "ounces swordfish snapper other firm fleshed fish\n",
      "tablespoon minced basil leaves\n",
      "handful fresh parsley finely minced\n",
      "ounces lard butter plus more brushing tops\n",
      "green cardamom pods\n",
      "stick pound unsalted butter softened\n",
      "teaspoon pepper flakes preferably turkish aleppo note more taste\n",
      "tablespoon fresh lemon juice\n",
      "scallions thinly sliced\n"
     ]
    }
   ],
   "source": [
    "# токенизируем\n",
    "train_tokenized = tokenize_corpus(train_source)\n",
    "test_tokenized = tokenize_corpus(test_source)\n",
    "print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:32.325205Z",
     "start_time": "2019-10-29T19:19:32.140837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря 2267\n",
      "[('<PAD>', 0), ('tablespoons', 1), ('teaspoon', 2), ('chopped', 3), ('salt', 4), ('pepper', 5), ('cups', 6), ('ground', 7), ('fresh', 8), ('tablespoon', 9)]\n"
     ]
    }
   ],
   "source": [
    "# строим словарь\n",
    "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=0.9, min_count=5, pad_word='<PAD>')\n",
    "print(\"Размер словаря\", len(vocabulary))\n",
    "print(list(vocabulary.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:32.686258Z",
     "start_time": "2019-10-29T19:19:32.327711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 52\n",
      "22 878 574 127 246 707 181\n",
      "9 19 88 33\n",
      "517 8 43 15 19\n",
      "22 586 20 45 47 649 648\n",
      "59 329 535\n",
      "200 12 50 20 266\n",
      "2 5 140 78 1208 735 153 47 10\n",
      "9 8 31 25\n",
      "98 65 27\n"
     ]
    }
   ],
   "source": [
    "# отображаем в номера токенов\n",
    "train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n",
    "test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n",
    "\n",
    "print('\\n'.join(' '.join(str(t) for t in sent)\n",
    "                for sent in train_token_ids[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:32.967989Z",
     "start_time": "2019-10-29T19:19:32.688319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(s) for s in train_token_ids], bins=20);\n",
    "plt.title('Гистограмма длин предложений');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:33.001487Z",
     "start_time": "2019-10-29T19:19:32.970153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([222,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0]), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCE_LEN = 20\n",
    "train_dataset = PaddedSequenceDataset(train_token_ids,\n",
    "                                      np.zeros(len(train_token_ids)),\n",
    "                                      out_len=MAX_SENTENCE_LEN)\n",
    "test_dataset = PaddedSequenceDataset(test_token_ids,\n",
    "                                     np.zeros(len(test_token_ids)),\n",
    "                                     out_len=MAX_SENTENCE_LEN)\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм обучения - Skip Gram Negative Sampling\n",
    "\n",
    "**Skip Gram** - предсказываем соседние слова по центральному слову\n",
    "\n",
    "**Negative Sampling** - аппроксимация softmax\n",
    "\n",
    "$$ W, D \\in \\mathbb{R}^{Vocab \\times EmbSize} $$\n",
    "\n",
    "$$ \\sum_{CenterW_i} P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) \\rightarrow \\max_{W,D} $$\n",
    "\n",
    "$$ P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) = \\prod_j P(CtxW_j | CenterW_i; W, D) $$\n",
    "    \n",
    "$$ P(CtxW_j | CenterW_i; W, D) = \\frac{e^{w_i \\cdot d_j}} { \\sum_{j=1}^{|V|} e^{w_i \\cdot d_j}} = softmax \\simeq \\frac{e^{w_i \\cdot d_j^+}} { \\sum_{j=1}^{k} e^{w_i \\cdot d_j^-}}, \\quad k \\ll |V| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:33.065376Z",
     "start_time": "2019-10-29T19:19:33.003081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 1., 1., 1., 0., 0., 0.],\n",
       "        [0., 1., 1., 1., 0., 1., 1., 1., 0., 0.],\n",
       "        [0., 0., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
       "        [0., 0., 0., 1., 1., 1., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_diag_mask(size, radius):\n",
    "    \"\"\"Квадратная матрица размера Size x Size с двумя полосами ширины radius вдоль главной диагонали\"\"\"\n",
    "    idxs = torch.arange(size)\n",
    "    abs_idx_diff = (idxs.unsqueeze(0) - idxs.unsqueeze(1)).abs()\n",
    "    mask = ((abs_idx_diff <= radius) & (abs_idx_diff > 0)).float()\n",
    "    return mask\n",
    "\n",
    "make_diag_mask(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:33.101379Z",
     "start_time": "2019-10-29T19:19:33.068154Z"
    }
   },
   "outputs": [],
   "source": [
    "class SkipGramNegativeSamplingTrainer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, sentence_len, radius=5, negative_samples_n=5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.negative_samples_n = negative_samples_n\n",
    "\n",
    "        self.center_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n",
    "        self.center_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
    "        self.center_emb.weight.data[0] = 0\n",
    "\n",
    "        self.context_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)        \n",
    "        self.context_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
    "        self.context_emb.weight.data[0] = 0\n",
    "\n",
    "        self.positive_sim_mask = make_diag_mask(sentence_len, radius) # MaxSentLength x MaxSentLength\n",
    "    \n",
    "    def forward(self, sentences):\n",
    "        \"\"\"sentences - Batch x MaxSentLength - идентификаторы токенов\"\"\"\n",
    "        batch_size = sentences.shape[0]\n",
    "        center_embeddings = self.center_emb(sentences)  # Batch x MaxSentLength x EmbSize\n",
    "\n",
    "        # оценить сходство с настоящими соседними словами\n",
    "        positive_context_embs = self.context_emb(sentences).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n",
    "        positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x MaxSentLength x MaxSentLength\n",
    "        positive_probs = torch.sigmoid(positive_sims)\n",
    "\n",
    "        # увеличить оценку вероятности встретить эти пары слов вместе\n",
    "        positive_mask = self.positive_sim_mask.to(positive_sims.device)\n",
    "        positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n",
    "                                               positive_mask.expand_as(positive_probs)) # Batch x MaxSentLength x MaxSentLength\n",
    "\n",
    "        # выбрать случайные \"отрицательные\" слова\n",
    "        negative_words = torch.randint(1, self.vocab_size,\n",
    "                                       size=(batch_size, self.negative_samples_n),\n",
    "                                       device=sentences.device)  # Batch x NegSamplesN\n",
    "        negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n",
    "        negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x MaxSentLength x NegSamplesN\n",
    "        \n",
    "        # уменьшить оценку вероятность встретить эти пары слов вместе\n",
    "        negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n",
    "                                                           negative_sims.new_zeros(negative_sims.shape))\n",
    "\n",
    "        return positive_loss + negative_loss\n",
    "\n",
    "\n",
    "def no_loss(pred, target):\n",
    "    \"\"\"Фиктивная функция потерь - когда модель сама считает функцию потерь\"\"\"\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_diag_mask(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[0, 1, 0,],\n",
    "              [1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [1., 0., 1.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_diag_mask(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 3].  Tensor sizes: [2, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c2937260cceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_diag_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 3].  Tensor sizes: [2, 2]"
     ]
    }
   ],
   "source": [
    "make_diag_mask(2, 1).expand_as(torch.tensor([[0, 1, 0,], [1, 0, 3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:33.130307Z",
     "start_time": "2019-10-29T19:19:33.103036Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), 100, MAX_SENTENCE_LEN,\n",
    "                                          radius=5, negative_samples_n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.830221Z",
     "start_time": "2019-10-29T19:19:33.132062Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_val_loss, best_model = train_eval_loop(trainer,\n",
    "                                            train_dataset,\n",
    "                                            test_dataset,\n",
    "                                            no_loss,\n",
    "                                            lr=1e-2,\n",
    "                                            epoch_n=2,\n",
    "                                            batch_size=8,\n",
    "                                            device='cpu',\n",
    "                                            early_stopping_patience=10,\n",
    "                                            max_batches_per_epoch_train=2000,\n",
    "                                            max_batches_per_epoch_val=len(test_dataset),\n",
    "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=1, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.862018Z",
     "start_time": "2019-10-29T19:20:12.832046Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(trainer.state_dict(), 'models/sgns.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.888270Z",
     "start_time": "2019-10-29T19:20:12.864706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "trainer.load_state_dict(torch.load('models/sgns.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследуем характеристики полученных векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.919904Z",
     "start_time": "2019-10-29T19:20:12.890671Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.942708Z",
     "start_time": "2019-10-29T19:20:12.921619Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings.most_similar('chicken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.965936Z",
     "start_time": "2019-10-29T19:20:12.944423Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings.analogy('cake', 'cacao', 'cheese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.991060Z",
     "start_time": "2019-10-29T19:20:12.967532Z"
    }
   },
   "outputs": [],
   "source": [
    "test_words = ['salad', 'fish', 'salmon', 'sauvignon', 'beef', 'pork', 'steak', 'beer', 'cake', 'coffee', 'sausage', 'wine', 'merlot', 'zinfandel', 'trout', 'chardonnay', 'champagne', 'cacao']\n",
    "test_vectors = embeddings.get_vectors(*test_words)\n",
    "print(test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:13.318676Z",
     "start_time": "2019-10-29T19:20:12.996595Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((10, 10))\n",
    "plot_vectors(test_vectors, test_words, how='svd', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение Word2Vec с помощью Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:13.613797Z",
     "start_time": "2019-10-29T19:20:13.321353Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:17.075005Z",
     "start_time": "2019-10-29T19:20:13.615729Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec(sentences=train_tokenized, size=100,\n",
    "                                  window=5, min_count=5, workers=4,\n",
    "                                  sg=1, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:17.109583Z",
     "start_time": "2019-10-29T19:20:17.076599Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec.wv.most_similar('chicken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:17.176357Z",
     "start_time": "2019-10-29T19:20:17.112948Z"
    }
   },
   "outputs": [],
   "source": [
    "gensim_words = [w for w in test_words if w in word2vec.wv.vocab]\n",
    "gensim_vectors = np.stack([word2vec.wv[w] for w in gensim_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:17.428874Z",
     "start_time": "2019-10-29T19:20:17.179311Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((10, 10))\n",
    "plot_vectors(gensim_vectors, test_words, how='svd', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка предобученного Word2Vec\n",
    "\n",
    "Источники готовых векторов:\n",
    "\n",
    "https://rusvectores.org/ru/ - для русского языка\n",
    "\n",
    "https://wikipedia2vec.github.io/wikipedia2vec/pretrained/ - много разных языков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:17.460133Z",
     "start_time": "2019-10-29T19:20:17.430563Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:17.980509Z",
     "start_time": "2019-10-29T19:20:17.462239Z"
    }
   },
   "outputs": [],
   "source": [
    "available_models = api.info()['models'].keys()\n",
    "print('\\n'.join(available_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:22:12.649035Z",
     "start_time": "2019-10-29T19:20:17.984118Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pretrained = api.load('word2vec-google-news-300')  # > 1.5 GB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:22:12.651388Z",
     "start_time": "2019-10-29T19:19:29.817Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained.most_similar('cheese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:22:12.652649Z",
     "start_time": "2019-10-29T19:19:29.820Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained.most_similar(positive=['man', 'queen'], negative=['king'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:22:12.653584Z",
     "start_time": "2019-10-29T19:19:29.823Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_words = [w for w in test_words if w in pretrained.vocab]\n",
    "pretrained_vectors = np.stack([pretrained[w] for w in pretrained_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:22:12.654594Z",
     "start_time": "2019-10-29T19:19:29.828Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((10, 10))\n",
    "plot_vectors(pretrained_vectors, test_words, how='svd', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "* Реализовали Skip Gram Negative Sampling на PyTorch\n",
    "* Обучили на корпусе рецептов\n",
    "    * Сходство слов модель выучила неплохо\n",
    "    * Для аналогий мало данных\n",
    "* Обучили SGNS с помощью библиотеки Gensim\n",
    "* Загрузили веса Word2Vec, полученные с помощью большого корпуса (GoogleNews)\n",
    "    * Списки похожих слов отличаются!\n",
    "    * Аналогии работают"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
